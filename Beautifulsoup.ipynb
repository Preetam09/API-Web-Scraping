{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1119ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba27c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '<!DOCTYPE html><html><head><title>Learning Beautiful Soup</title></head>\\\n",
    "<body><h1> About Us </h1><div class = \"first_div\"><p>Coding Ninjas Website</p>\\\n",
    "<a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a>\\\n",
    "<ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul>\\\n",
    "</div><p id = \"template_p\">This is a template paragraph tag</p>\\\n",
    "<a href = \"https://www.facebook.com/codingninjas/\">\\\n",
    "This is the link of our Facebook Page</a></body></html>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f275fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Learning Beautiful Soup\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   About Us\n",
      "  </h1>\n",
      "  <div class=\"first_div\">\n",
      "   <p>\n",
      "    Coding Ninjas Website\n",
      "   </p>\n",
      "   <a href=\"https://www.codingninjas.in/\">\n",
      "    Link to Coding Ninjas Website\n",
      "   </a>\n",
      "   <ul>\n",
      "    <li>\n",
      "     This\n",
      "    </li>\n",
      "    <li>\n",
      "     is\n",
      "    </li>\n",
      "    <li>\n",
      "     an\n",
      "    </li>\n",
      "    <li>\n",
      "     unordered\n",
      "    </li>\n",
      "    <li>\n",
      "     list.\n",
      "    </li>\n",
      "   </ul>\n",
      "  </div>\n",
      "  <p id=\"template_p\">\n",
      "   This is a template paragraph tag\n",
      "  </p>\n",
      "  <a href=\"https://www.facebook.com/codingninjas/\">\n",
      "   This is the link of our Facebook Page\n",
      "  </a>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "data = bs(html,'html.parser')\n",
    "print(data.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "168eb7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body><h1> About Us </h1><div class=\"first_div\"><p>Coding Ninjas Website</p><a href=\"https://www.codingninjas.in/\">Link to Coding Ninjas Website</a><ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul></div><p id=\"template_p\">This is a template paragraph tag</p><a href=\"https://www.facebook.com/codingninjas/\">This is the link of our Facebook Page</a></body>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8a7d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n"
     ]
    }
   ],
   "source": [
    "d = data.div.attrs\n",
    "for i in d:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "726f2e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an unordered list. "
     ]
    }
   ],
   "source": [
    "ans = data.find_all('li')\n",
    "for i in ans:\n",
    "    print(i.string,end = ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "687816c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.codingninjas.in/\n",
      "https://www.facebook.com/codingninjas/\n"
     ]
    }
   ],
   "source": [
    "ans = data.find_all('a')\n",
    "for i in ans:\n",
    "    a = i.attrs\n",
    "    for j in a:\n",
    "        print(a[j])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1983c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '<!DOCTYPE html><html><head><title>Navigate Parse Tree</title></head>\\\n",
    "<body><h1>This is your Assignment</h1><a href = \"https://www.google.com\">This is a link that will take you to Google</a>\\\n",
    "<ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p>\\\n",
    "<p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li>\\\n",
    "<li id = \"li2\">This is an li tag given to you for scraping</li>\\\n",
    "<li>This li tag gives you the various ways to get data from a website\\\n",
    "<ol><li class = \"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li>\\\n",
    "<li>Scrape data using Scrapy</li></ol></li>\\\n",
    "<li class = \"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">\\\n",
    "Clicking on this takes you to the documentation of BeautifulSoup</a>\\\n",
    "<a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a>\\\n",
    "</li></ul></body></html>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9cb97a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "dd = bs(html,'html.parser')\n",
    "d = len(list(dd.html.descendants))\n",
    "c = len(list(dd.html.children))\n",
    "print(d-c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "47466c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "li\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "l = dd.html.descendants\n",
    "for i in l:\n",
    "    if i.name != None:\n",
    "        if 'id' in i.attrs:\n",
    "            print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a4c29b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Navigate Parse Tree\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   This is your Assignment\n",
      "  </h1>\n",
      "  <a href=\"https://www.google.com\">\n",
      "   This is a link that will take you to Google\n",
      "  </a>\n",
      "  <ul>\n",
      "   <li>\n",
      "    <p>\n",
      "     This question is given to test your knowledge of\n",
      "     <b>\n",
      "      Web Scraping\n",
      "     </b>\n",
      "    </p>\n",
      "    <p>\n",
      "     Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.\n",
      "    </p>\n",
      "   </li>\n",
      "   <li id=\"li2\">\n",
      "    This is an li tag given to you for scraping\n",
      "   </li>\n",
      "   <li>\n",
      "    This li tag gives you the various ways to get data from a website\n",
      "    <ol>\n",
      "     <li class=\"list_or\">\n",
      "      Using API of the website\n",
      "     </li>\n",
      "     <li>\n",
      "      Scrape data using BeautifulSoup\n",
      "     </li>\n",
      "     <li>\n",
      "      Scrape data using Selenium\n",
      "     </li>\n",
      "     <li>\n",
      "      Scrape data using Scrapy\n",
      "     </li>\n",
      "    </ol>\n",
      "   </li>\n",
      "   <li class=\"list_or\">\n",
      "    <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">\n",
      "     Clicking on this takes you to the documentation of BeautifulSoup\n",
      "    </a>\n",
      "    <a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">\n",
      "     Clicking on this takes you to the documentation of Selenium\n",
      "    </a>\n",
      "   </li>\n",
      "  </ul>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(dd.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "acd5dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'id': 'li2'}\n",
      "{}\n",
      "{'class': ['list_or']}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{'class': ['list_or']}\n"
     ]
    }
   ],
   "source": [
    "l = dd.find_all(id = 'li2')\n",
    "s= l[0].name\n",
    "\n",
    "for i in dd.find_all('li'):\n",
    "    print(i.attrs)\n",
    "#for i in dd.li.next_siblings:\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "29d377af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head><title>Navigate Parse Tree</title></head>\n",
      "<html><head><title>Navigate Parse Tree</title></head><body><h1>This is your Assignment</h1><a href=\"https://www.google.com\">This is a link that will take you to Google</a><ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p><p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li><li id=\"li2\">This is an li tag given to you for scraping</li><li>This li tag gives you the various ways to get data from a website<ol><li class=\"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li><li>Scrape data using Scrapy</li></ol></li><li class=\"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Clicking on this takes you to the documentation of BeautifulSoup</a><a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a></li></ul></body></html>\n",
      "<!DOCTYPE html>\n",
      "<html><head><title>Navigate Parse Tree</title></head><body><h1>This is your Assignment</h1><a href=\"https://www.google.com\">This is a link that will take you to Google</a><ul><li><p> This question is given to test your knowledge of <b>Web Scraping</b></p><p>Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.</p></li><li id=\"li2\">This is an li tag given to you for scraping</li><li>This li tag gives you the various ways to get data from a website<ol><li class=\"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li><li>Scrape data using Scrapy</li></ol></li><li class=\"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Clicking on this takes you to the documentation of BeautifulSoup</a><a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a></li></ul></body></html>\n"
     ]
    }
   ],
   "source": [
    "for i in dd.title.parents:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9224a0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li>This li tag gives you the various ways to get data from a website<ol><li class=\"list_or\">Using API of the website</li><li>Scrape data using BeautifulSoup</li><li>Scrape data using Selenium</li><li>Scrape data using Scrapy</li></ol></li>\n",
      "<li class=\"list_or\"><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Clicking on this takes you to the documentation of BeautifulSoup</a><a href=\"https://selenium-python.readthedocs.io/\" id=\"anchor\">Clicking on this takes you to the documentation of Selenium</a></li>\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for i in dd.find_all('li'):\n",
    "    a = i.attrs\n",
    "    for j in a:\n",
    "        if a[j] == 'li2':\n",
    "            for k in i.next_siblings:\n",
    "                print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "13a8d3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.google.com'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1e88a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46f90871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic\n",
      "Tipping the Velvet\n",
      "Soumission\n",
      "Sharp Objects\n",
      "Sapiens: A Brief History of Humankind\n",
      "The Requiem Red\n",
      "The Dirty Little Secrets of Getting Your Dream Job\n",
      "The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\n",
      "The Black Maria\n",
      "Starving Hearts (Triangular Trade Trilogy, #1)\n",
      "Shakespeare's Sonnets\n",
      "Set Me Free\n",
      "Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\n",
      "Rip it Up and Start Again\n",
      "Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\n",
      "Olio\n",
      "Mesaerion: The Best Science Fiction Stories 1800-1849\n",
      "Libertarianism for Beginners\n",
      "It's Only the Himalayas\n"
     ]
    }
   ],
   "source": [
    "html = requests.get('http://books.toscrape.com/')\n",
    "data = bs(html.text,'html.parser')\n",
    "name = data.find_all(class_= 'product_pod')\n",
    "for i in name:\n",
    "    print(i.h3.a['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6fb4d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel\n",
      "Mystery\n",
      "Historical Fiction\n",
      "Sequential Art\n",
      "Classics\n",
      "Philosophy\n",
      "Romance\n",
      "Womens Fiction\n",
      "Fiction\n",
      "Childrens\n",
      "Religion\n",
      "Nonfiction\n",
      "Music\n",
      "Default\n",
      "Science Fiction\n",
      "Sports and Games\n",
      "Add a comment\n",
      "Fantasy\n",
      "New Adult\n",
      "Young Adult\n",
      "Science\n",
      "Poetry\n",
      "Paranormal\n",
      "Art\n",
      "Psychology\n",
      "Autobiography\n",
      "Parenting\n",
      "Adult Fiction\n",
      "Humor\n",
      "Horror\n",
      "History\n",
      "Food and Drink\n",
      "Christian Fiction\n",
      "Business\n",
      "Biography\n",
      "Thriller\n",
      "Contemporary\n",
      "Spirituality\n",
      "Academic\n",
      "Self Help\n",
      "Historical\n",
      "Christian\n",
      "Suspense\n",
      "Short Stories\n",
      "Novels\n",
      "Health\n",
      "Politics\n",
      "Cultural\n",
      "Erotica\n",
      "Crime\n"
     ]
    }
   ],
   "source": [
    "html = requests.get('http://books.toscrape.com/')\n",
    "data = bs(html.text,'html.parser')\n",
    "name = data.find(class_= 'side_categories')\n",
    "r = name.find_all('a')\n",
    "for i in r[1:]:\n",
    "    ans = i.string\n",
    "    print(ans.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec7d05e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(1,51):\n",
    "    url = 'http://books.toscrape.com/catalogue/page-'+str(i)+'.html'\n",
    "    html = requests.get(url)\n",
    "    data = bs(html.text,'html.parser')\n",
    "    name = data.find_all(class_= 'product_pod')\n",
    "    a = a+len(name)\n",
    "    for i in name:\n",
    "        #print(i.h3.a['title'])\n",
    "        pass\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6e99787",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_books' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-dda56b5673ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'http://books.toscrape.com/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_books\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0murls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# urls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_books' is not defined"
     ]
    }
   ],
   "source": [
    "base_url = 'http://books.toscrape.com/'\n",
    "urls = []\n",
    "for i in all_books : \n",
    "    urls.append(base_url + i.a['href'])\n",
    "# urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d029075b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-7c445cf9caba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mnext_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'next'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnext_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "all_urls = ['http://books.toscrape.com/catalogue/page-1.html']\n",
    "current_page = 'http://books.toscrape.com/catalogue/page-1.html'\n",
    "\n",
    "base_url = 'http://books.toscrape.com/catalogue/'\n",
    "\n",
    "response = requests.get(current_page)\n",
    "\n",
    "while response.status_code == 200 :\n",
    "    data = BeautifulSoup(response.text, 'html.parser')\n",
    "    next_page = data.find(class_='next')\n",
    "    if next_page is None : \n",
    "        break\n",
    "    next_page_url = base_url + next_page.a['href']\n",
    "    print(next_page_url)\n",
    "    all_urls.append(next_page_url)\n",
    "    \n",
    "    current_page = next_page_url\n",
    "    response = requests.get(current_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09ca94df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Server': 'nginx/1.17.7', 'Date': 'Wed, 26 Jan 2022 05:35:54 GMT', 'Content-Type': 'text/html', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding', 'Last-Modified': 'Thu, 25 Mar 2021 13:59:05 GMT', 'Content-Encoding': 'gzip'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b23e10df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html 51.77 22\n",
      "Tipping the Velvet http://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html 53.74 20\n",
      "Soumission http://books.toscrape.com/catalogue/soumission_998/index.html 50.1 20\n",
      "Sharp Objects http://books.toscrape.com/catalogue/sharp-objects_997/index.html 47.82 20\n",
      "Sapiens: A Brief History of Humankind http://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html 54.23 20\n",
      "The Requiem Red http://books.toscrape.com/catalogue/the-requiem-red_995/index.html 22.65 19\n",
      "The Dirty Little Secrets of Getting Your Dream Job http://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html 33.34 19\n",
      "The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull http://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html 17.93 19\n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics http://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html 22.6 19\n",
      "The Black Maria http://books.toscrape.com/catalogue/the-black-maria_991/index.html 52.15 19\n",
      "Starving Hearts (Triangular Trade Trilogy, #1) http://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html 13.99 19\n",
      "Shakespeare's Sonnets http://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html 20.66 19\n",
      "Set Me Free http://books.toscrape.com/catalogue/set-me-free_988/index.html 17.46 19\n",
      "Scott Pilgrim's Precious Little Life (Scott Pilgrim #1) http://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html 52.29 19\n",
      "Rip it Up and Start Again http://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html 35.02 19\n",
      "Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991 http://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html 57.25 19\n",
      "Olio http://books.toscrape.com/catalogue/olio_984/index.html 23.88 19\n",
      "Mesaerion: The Best Science Fiction Stories 1800-1849 http://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html 37.59 19\n",
      "Libertarianism for Beginners http://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html 51.33 19\n",
      "It's Only the Himalayas http://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html 45.17 19\n",
      "In Her Wake http://books.toscrape.com/catalogue/in-her-wake_980/index.html 12.84 19\n",
      "How Music Works http://books.toscrape.com/catalogue/how-music-works_979/index.html 37.32 19\n",
      "Foolproof Preserving: A Guide to Small Batch Jams, Jellies, Pickles, Condiments, and More: A Foolproof Guide to Making Small Batch Jams, Jellies, Pickles, Condiments, and More http://books.toscrape.com/catalogue/foolproof-preserving-a-guide-to-small-batch-jams-jellies-pickles-condiments-and-more-a-foolproof-guide-to-making-small-batch-jams-jellies-pickles-condiments-and-more_978/index.html 30.52 19\n",
      "Chase Me (Paris Nights #2) http://books.toscrape.com/catalogue/chase-me-paris-nights-2_977/index.html 25.27 19\n",
      "Black Dust http://books.toscrape.com/catalogue/black-dust_976/index.html 34.53 19\n",
      "Birdsong: A Story in Pictures http://books.toscrape.com/catalogue/birdsong-a-story-in-pictures_975/index.html 54.64 19\n",
      "America's Cradle of Quarterbacks: Western Pennsylvania's Football Factory from Johnny Unitas to Joe Montana http://books.toscrape.com/catalogue/americas-cradle-of-quarterbacks-western-pennsylvanias-football-factory-from-johnny-unitas-to-joe-montana_974/index.html 22.5 19\n",
      "Aladdin and His Wonderful Lamp http://books.toscrape.com/catalogue/aladdin-and-his-wonderful-lamp_973/index.html 53.13 19\n",
      "Worlds Elsewhere: Journeys Around Shakespeareâs Globe http://books.toscrape.com/catalogue/worlds-elsewhere-journeys-around-shakespeares-globe_972/index.html 40.3 18\n",
      "Wall and Piece http://books.toscrape.com/catalogue/wall-and-piece_971/index.html 44.18 18\n",
      "The Four Agreements: A Practical Guide to Personal Freedom http://books.toscrape.com/catalogue/the-four-agreements-a-practical-guide-to-personal-freedom_970/index.html 17.66 18\n",
      "The Five Love Languages: How to Express Heartfelt Commitment to Your Mate http://books.toscrape.com/catalogue/the-five-love-languages-how-to-express-heartfelt-commitment-to-your-mate_969/index.html 31.05 18\n",
      "The Elephant Tree http://books.toscrape.com/catalogue/the-elephant-tree_968/index.html 23.82 18\n",
      "The Bear and the Piano http://books.toscrape.com/catalogue/the-bear-and-the-piano_967/index.html 36.89 18\n",
      "Sophie's World http://books.toscrape.com/catalogue/sophies-world_966/index.html 15.94 18\n",
      "Penny Maybe http://books.toscrape.com/catalogue/penny-maybe_965/index.html 33.29 18\n",
      "Maude (1883-1993):She Grew Up with the country http://books.toscrape.com/catalogue/maude-1883-1993she-grew-up-with-the-country_964/index.html 18.02 18\n",
      "In a Dark, Dark Wood http://books.toscrape.com/catalogue/in-a-dark-dark-wood_963/index.html 19.63 18\n",
      "Behind Closed Doors http://books.toscrape.com/catalogue/behind-closed-doors_962/index.html 52.22 18\n",
      "You can't bury them all: Poems http://books.toscrape.com/catalogue/you-cant-bury-them-all-poems_961/index.html 33.63 17\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import re\n",
    "allPages = ['http://books.toscrape.com/catalogue/page-1.html',\n",
    "            'http://books.toscrape.com/catalogue/page-2.html']\n",
    "b = []\n",
    "column_names = ['Title', 'Link', 'Price', 'Quantity in Stock']\n",
    "for i in allPages:\n",
    "    page = requests.get(i)\n",
    "    cat = bs(page.text,'html.parser')\n",
    "    for j in cat.find_all(class_ = 'product_pod'):\n",
    "        url = j.a['href']\n",
    "        book = requests.get('http://books.toscrape.com/catalogue/'+url)\n",
    "        ans = bs(book.text,'html.parser')\n",
    "        title = ans.h1.string\n",
    "        b_url = 'http://books.toscrape.com/catalogue/'+url\n",
    "        price = float(ans.find(class_ = 'price_color').string[2:])\n",
    "        stock = ans.find(class_ = 'instock availability').contents[-1].strip()\n",
    "        s = int(stock[-13:-10])\n",
    "        print(title,b_url,price,s)\n",
    "        b.append([title,b_url,price,s])\n",
    "ans = pd.DataFrame(b,columns = column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c89a42b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avengers: Infinity War : Action, Adventure, Sci-Fi\n",
      "Black Panther : Action, Adventure, Sci-Fi\n",
      "Deadpool 2 : Action, Adventure, Comedy\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "html = requests.get('https://www.imdb.com/search/title?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt')\n",
    "data = bs(html.text,'html.parser')\n",
    "movie = data.find_all(class_ = 'lister-item-content')\n",
    "for i in movie[0:3]:\n",
    "    name = i.find('a').string\n",
    "    genre = i.find(class_ = 'genre').string.strip()\n",
    "    print(name,':',genre)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5315ddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception\n",
      "Game of Thrones\n",
      "The Dark Knight Rises\n",
      "The Wolf of Wall Street\n",
      "Interstellar\n"
     ]
    }
   ],
   "source": [
    "vote_url = ['https://www.imdb.com/search/title/?release_date=2010&sort=num_votes,desc&page=1&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2011&sort=num_votes,desc&page=1&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2012&sort=num_votes,desc&page=1&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2013&sort=num_votes,desc&page=1&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2014&sort=num_votes,desc&page=1&ref_=adv_nxt']\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "for i in vote_url:\n",
    "    html = requests.get(i)\n",
    "    data = bs(html.text,'html.parser')\n",
    "    movie = data.find(class_ = 'lister-item-content')\n",
    "    print(movie.a.string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4febb5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Haunting of Hill House 572 min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "url = ['https://www.imdb.com/search/title/?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&start=1&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&start=51&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&start=101&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&start=151&ref_=adv_nxt','https://www.imdb.com/search/title/?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&start=201&ref_=adv_nxt']\n",
    "time = 0\n",
    "n_ans = None\n",
    "t = 0\n",
    "for i in url:\n",
    "    html = requests.get(i)\n",
    "    data = bs(html.text,'html.parser')\n",
    "    movie = data.find_all(class_ = 'lister-item-content')\n",
    "    for i in movie:\n",
    "        if i.p.find(class_ = 'runtime') == None:\n",
    "            continue\n",
    "        s = i.p.find(class_ = 'runtime').string.split()\n",
    "        time = int(s[0])\n",
    "        if time>t:\n",
    "            t = time\n",
    "            n_ans = i.h3.a.string\n",
    "        pass\n",
    "print(n_ans,str(t)+' min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "178ee507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Artificial_intelligence//upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "html = requests.get('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "data = bs(html.text,'html.parser')\n",
    "ans = None\n",
    "a = 0\n",
    "for i in data.find_all('img'):\n",
    "    d = i.attrs\n",
    "    if 'height' in d and 'width' in d:\n",
    "        h = int(d['height'])\n",
    "        w = int(d['width'])\n",
    "        arr = h*w\n",
    "        if arr>a:\n",
    "            a = arr\n",
    "            ans = d['src']\n",
    "    pass\n",
    "print('https://en.wikipedia.org/wiki/Artificial_intelligence'+ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "41856dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "“A day without sunshine is like, you know, night.”\n",
      "“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”\n",
      "“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”\n",
      "“All you need is love. But a little chocolate now and then doesn't hurt.”\n",
      "“Remember, we're madly in love, so it's all right to kiss me anytime you feel like it.”\n",
      "“Some people never go crazy. What truly horrible lives they must lead.”\n",
      "“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”\n",
      "“Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”\n",
      "“The reason I talk to myself is because I’m the only one whose answers I accept.”\n",
      "“I am free of all prejudice. I hate everyone equally. ”\n",
      "“A lady's imagination is very rapid; it jumps from admiration to love, from love to matrimony in a moment.”\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "url = ['http://quotes.toscrape.com/page/1/','http://quotes.toscrape.com/page/2/','http://quotes.toscrape.com/page/3/','http://quotes.toscrape.com/page/4/','http://quotes.toscrape.com/page/5/','http://quotes.toscrape.com/page/6/','http://quotes.toscrape.com/page/7/','http://quotes.toscrape.com/page/8/','http://quotes.toscrape.com/page/9/','http://quotes.toscrape.com/page/10/']\n",
    "for i in url:\n",
    "    html = requests.get(i)\n",
    "    data = bs(html.text,'html.parser')\n",
    "    tags= data.find_all(class_ = 'keywords')\n",
    "    quotes = data.find_all(class_ = 'text')\n",
    "    for i in range(len(tags)):\n",
    "        if 'humor' in tags[i]['content']:\n",
    "            print(quotes[i].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1e725fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein\n",
      "Alexandre Dumas fils\n",
      "Alfred Tennyson\n",
      "Allen Saunders\n",
      "André Gide\n",
      "Ayn Rand\n",
      "Bob Marley\n",
      "C.S. Lewis\n",
      "Charles Bukowski\n",
      "Charles M. Schulz\n",
      "Douglas Adams\n",
      "Dr. Seuss\n",
      "E.E. Cummings\n",
      "Eleanor Roosevelt\n",
      "Elie Wiesel\n",
      "Ernest Hemingway\n",
      "Friedrich Nietzsche\n",
      "Garrison Keillor\n",
      "George Bernard Shaw\n",
      "George Carlin\n",
      "George Eliot\n",
      "George R.R. Martin\n",
      "Harper Lee\n",
      "Haruki Murakami\n",
      "Helen Keller\n",
      "J.D. Salinger\n",
      "J.K. Rowling\n",
      "J.M. Barrie\n",
      "J.R.R. Tolkien\n",
      "James Baldwin\n",
      "Jane Austen\n",
      "Jim Henson\n",
      "Jimi Hendrix\n",
      "John Lennon\n",
      "Jorge Luis Borges\n",
      "Khaled Hosseini\n",
      "Madeleine L'Engle\n",
      "Marilyn Monroe\n",
      "Mark Twain\n",
      "Martin Luther King Jr.\n",
      "Mother Teresa\n",
      "Pablo Neruda\n",
      "Ralph Waldo Emerson\n",
      "Stephenie Meyer\n",
      "Steve Martin\n",
      "Suzanne Collins\n",
      "Terry Pratchett\n",
      "Thomas A. Edison\n",
      "W.C. Fields\n",
      "William Nicholson\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "url = ['http://quotes.toscrape.com/page/1/','http://quotes.toscrape.com/page/2/','http://quotes.toscrape.com/page/3/','http://quotes.toscrape.com/page/4/','http://quotes.toscrape.com/page/5/','http://quotes.toscrape.com/page/6/','http://quotes.toscrape.com/page/7/','http://quotes.toscrape.com/page/8/','http://quotes.toscrape.com/page/9/','http://quotes.toscrape.com/page/10/']\n",
    "d = {}\n",
    "for i in url:\n",
    "    html = requests.get(i)\n",
    "    data = bs(html.text,'html.parser')\n",
    "    a = data.find_all(class_ = 'author')\n",
    "    for i in a:\n",
    "        d[i.string] = 0\n",
    "authors = []\n",
    "for i in d:\n",
    "    authors.append(i)\n",
    "authors.sort()\n",
    "for i in authors:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c9a6e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/Jane-Austen\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/Jim-Henson\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/Jorge-Luis-Borges\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/James-Baldwin\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/Jane-Austen\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/J-R-R-Tolkien\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/J-D-Salinger\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/John-Lennon\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/Jane-Austen\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/Jane-Austen\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/Jane-Austen\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/Jimi-Hendrix\">(about)</a>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<a href=\"/author/J-M-Barrie\">(about)</a>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "url_i='http://quotes.toscrape.com'\n",
    "url = ['http://quotes.toscrape.com/page/1/','http://quotes.toscrape.com/page/2/','http://quotes.toscrape.com/page/3/','http://quotes.toscrape.com/page/4/','http://quotes.toscrape.com/page/5/','http://quotes.toscrape.com/page/6/','http://quotes.toscrape.com/page/7/','http://quotes.toscrape.com/page/8/','http://quotes.toscrape.com/page/9/','http://quotes.toscrape.com/page/10/']\n",
    "ans = {}\n",
    "for k in url:\n",
    "    html = requests.get(k)\n",
    "    data = bs(html.text,'html.parser')\n",
    "    a = data.find_all(class_ = 'author')\n",
    "    b = data.find_all('a')\n",
    "    for i in a:\n",
    "        if i.string[0] == \"J\":\n",
    "            for j in i.next_siblings:\n",
    "                html_i = requests.get(url_i+j)\n",
    "                data_i = bs(html_i.text,'html.parser')\n",
    "                d = data_i.find(class_ = 'author-born-date')\n",
    "                print(d)\n",
    "                \n",
    "\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e5db3d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'J.D. Salinger': 'January 01, 1919',\n",
       " 'J.K. Rowling': 'July 31, 1965',\n",
       " 'J.M. Barrie': 'May 09, 1860',\n",
       " 'J.R.R. Tolkien': 'January 03, 1892',\n",
       " 'James Baldwin': 'August 02, 1924',\n",
       " 'Jane Austen': 'December 16, 1775',\n",
       " 'Jim Henson': 'September 24, 1936',\n",
       " 'Jimi Hendrix': 'November 27, 1942',\n",
       " 'John Lennon': 'October 09, 1940',\n",
       " 'Jorge Luis Borges': 'August 24, 1899'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "url_i='http://quotes.toscrape.com'\n",
    "url = ['http://quotes.toscrape.com/page/1/','http://quotes.toscrape.com/page/2/','http://quotes.toscrape.com/page/3/','http://quotes.toscrape.com/page/4/','http://quotes.toscrape.com/page/5/','http://quotes.toscrape.com/page/6/','http://quotes.toscrape.com/page/7/','http://quotes.toscrape.com/page/8/','http://quotes.toscrape.com/page/9/','http://quotes.toscrape.com/page/10/']\n",
    "ans = {}\n",
    "for k in url:\n",
    "    html = requests.get(k)\n",
    "    data = bs(html.text,'html.parser')\n",
    "    a = data.find_all(class_ = 'quote')\n",
    "    for i in a:\n",
    "        if i.find(class_ = 'author').string[0] == \"J\":\n",
    "            s =i.find('a').attrs['href']\n",
    "            html_i = requests.get(url_i+s)\n",
    "            data_i = bs(html_i.text,'html.parser')\n",
    "            d = data_i.find(class_ = 'author-born-date')\n",
    "            ans[i.find(class_ = 'author').string] = d.string\n",
    "            \n",
    "d = {}\n",
    "j = list(ans.keys())\n",
    "j.sort()\n",
    "for i in j:\n",
    "    d[i] = ans[i]\n",
    "d\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd456e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "“If you can't explain it to a six year old, you don't understand it yourself.”\n",
      "“If you want your children to be intelligent, read them fairy tales. If you want them to be more intelligent, read them more fairy tales.”\n",
      "“Logic will get you from A to Z; imagination will get you everywhere.”\n",
      "“Any fool can know. The point is to understand.”\n",
      "“Life is like riding a bicycle. To keep your balance, you must keep moving.”\n",
      "“If I were not a physicist, I would probably be a musician. I often think in music. I live my daydreams in music. I see my life in terms of music.”\n",
      "“Anyone who has never made a mistake has never tried anything new.”\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "url = ['http://quotes.toscrape.com/page/1/','http://quotes.toscrape.com/page/2/','http://quotes.toscrape.com/page/3/','http://quotes.toscrape.com/page/4/','http://quotes.toscrape.com/page/5/','http://quotes.toscrape.com/page/6/','http://quotes.toscrape.com/page/7/','http://quotes.toscrape.com/page/8/','http://quotes.toscrape.com/page/9/','http://quotes.toscrape.com/page/10/']\n",
    "for i in url:\n",
    "    html = requests.get(i)\n",
    "    data = bs(html.text,'html.parser')\n",
    "    a = data.find_all(class_ = 'author')\n",
    "    b = data.find_all(class_='text')\n",
    "    for i in range(len(a)):\n",
    "        if a[i].string == \"Albert Einstein\":\n",
    "            print(b[i].string)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaa91ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel\n",
      "Mystery\n",
      "Historical Fiction\n",
      "Sequential Art\n",
      "Classics\n",
      "Philosophy\n",
      "Romance\n",
      "Womens Fiction\n",
      "Fiction\n",
      "Childrens\n",
      "Religion\n",
      "Nonfiction\n",
      "Music\n",
      "Default\n",
      "Science Fiction\n",
      "Sports and Games\n",
      "Add a comment\n",
      "Fantasy\n",
      "New Adult\n",
      "Young Adult\n",
      "Science\n",
      "Poetry\n",
      "Paranormal\n",
      "Art\n",
      "Psychology\n",
      "Autobiography\n",
      "Parenting\n",
      "Adult Fiction\n",
      "Humor\n",
      "Horror\n",
      "History\n",
      "Food and Drink\n",
      "Christian Fiction\n",
      "Business\n",
      "Biography\n",
      "Thriller\n",
      "Contemporary\n",
      "Spirituality\n",
      "Academic\n",
      "Self Help\n",
      "Historical\n",
      "Christian\n",
      "Suspense\n",
      "Short Stories\n",
      "Novels\n",
      "Health\n",
      "Politics\n",
      "Cultural\n",
      "Erotica\n",
      "Crime\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "url = \"http://books.toscrape.com/\"\n",
    "html = requests.get(url)\n",
    "data = bs(html.text,'html.parser')\n",
    "a = data.find_all(class_='nav-list')\n",
    "b = a[0].find_all('a')\n",
    "for i in b[1:]:\n",
    "    for j in i.stripped_strings:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e6f963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Development Trends to watch out for in 2020\n",
      "Web Development: Interviews and You!\n",
      "Get equipped for the Technical Interviews\n",
      "Explore more about the projects in Web Development\n",
      "5G to be a major gamechanger for Edu-tech platforms\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "url = \"https://medium.com/codingninjas-blog\"\n",
    "html = requests.get(url)\n",
    "data = bs(html.text,'html.parser')\n",
    "a = data.find_all(class_='graf--title')\n",
    "for i in a[0:5]:\n",
    "    print(i.string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
